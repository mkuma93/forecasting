{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e177f6a5",
   "metadata": {},
   "source": [
    "# DeepSequence PWL Demo\n",
    "\n",
    "This notebook demonstrates the **DeepSequence PWL** model with modular component architecture.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Modular Component Design**: Each component (Trend, Seasonal, Holiday, Regressor) receives only its relevant features\n",
    "2. **Two Modes**:\n",
    "   - **Intermittent**: Two-stage prediction (zero probability + magnitude) for sparse demand\n",
    "   - **Continuous**: Direct forecasting for regular demand\n",
    "3. **Piecewise-Linear (PWL) Calibration**: Monotonic constraints for interpretability\n",
    "4. **No Transformers**: Efficient, lightweight architecture\n",
    "\n",
    "## Feature Separation Benefits\n",
    "\n",
    "- **No redundancy**: Each feature used by appropriate component only\n",
    "- **Better interpretability**: Clear component responsibilities\n",
    "- **Efficient learning**: No feature overweighting\n",
    "- **True modularity**: Components learn specialized patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04635246",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e0f184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n",
      "✓ Using modular component architecture with feature separation\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import from the new src-layout structure\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.deepsequence_pwl import DeepSequencePWL\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(\"✓ Using modular component architecture with feature separation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the module to get latest changes\n",
    "import importlib\n",
    "if 'deepsequence_pwl' in sys.modules:\n",
    "    importlib.reload(sys.modules['deepsequence_pwl.model'])\n",
    "    from src.deepsequence_pwl import DeepSequencePWL\n",
    "    print(\"✓ Module reloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382e3ef",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Data\n",
    "\n",
    "Let's create synthetic time series data with:\n",
    "- Multiple SKUs (products)\n",
    "- Trend, seasonality, and noise\n",
    "- High zero rate (sparse/intermittent demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03943217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created:\n",
      "  Samples: 10,000\n",
      "  SKUs: 25\n",
      "  Features: 6\n",
      "    - Trend features (0-1): time-based\n",
      "    - Seasonal features (2-3): cyclic patterns\n",
      "    - Regressor features (4-5): external variables\n",
      "  Zero rate: 90.2%\n",
      "  Non-zero mean: 11.63\n",
      "  Non-zero std: 4.49\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "n_samples = 10000\n",
    "n_skus = 25\n",
    "n_features = 6  # Increased to demonstrate feature separation\n",
    "zero_rate = 0.90  # 90% zeros (highly sparse)\n",
    "\n",
    "# Generate features with semantic meaning:\n",
    "# Features 0-1: Trend-related (time features)\n",
    "# Features 2-3: Seasonal-related (cyclic features)\n",
    "# Features 4-5: Regressor-related (external variables)\n",
    "time_index = np.arange(n_samples)\n",
    "X_trend = np.column_stack([\n",
    "    time_index / n_samples,  # Normalized time\n",
    "    (time_index % 30) / 30   # Month progress\n",
    "])\n",
    "X_seasonal = np.column_stack([\n",
    "    np.sin(2 * np.pi * time_index / 7),    # Day of week\n",
    "    np.cos(2 * np.pi * time_index / 365)   # Day of year\n",
    "])\n",
    "X_regressor = np.random.randn(n_samples, 2)  # External variables (price, promo, etc.)\n",
    "\n",
    "X = np.column_stack([X_trend, X_seasonal, X_regressor])\n",
    "\n",
    "# Generate SKU IDs\n",
    "sku_ids = np.random.randint(0, n_skus, n_samples)\n",
    "\n",
    "# Generate target with trend and seasonality\n",
    "trend = 0.01 * time_index / 100\n",
    "seasonality = 5 * np.sin(2 * np.pi * time_index / 365)\n",
    "noise = np.random.randn(n_samples) * 2\n",
    "\n",
    "y_magnitude = np.maximum(0, 10 + trend + seasonality + X.sum(axis=1) + noise)\n",
    "\n",
    "# Apply sparsity (intermittent demand)\n",
    "zero_mask = np.random.rand(n_samples) < zero_rate\n",
    "y = y_magnitude.copy()\n",
    "y[zero_mask] = 0\n",
    "\n",
    "print(f\"Dataset created:\")\n",
    "print(f\"  Samples: {n_samples:,}\")\n",
    "print(f\"  SKUs: {n_skus}\")\n",
    "print(f\"  Features: {n_features}\")\n",
    "print(f\"    - Trend features (0-1): time-based\")\n",
    "print(f\"    - Seasonal features (2-3): cyclic patterns\")\n",
    "print(f\"    - Regressor features (4-5): external variables\")\n",
    "print(f\"  Zero rate: {(y == 0).mean():.1%}\")\n",
    "print(f\"  Non-zero mean: {y[y > 0].mean():.2f}\")\n",
    "print(f\"  Non-zero std: {y[y > 0].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0987741",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "733ebe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split:\n",
      "  Train: 7,004 samples (70.0%)\n",
      "  Val:   1,496 samples (15.0%)\n",
      "  Test:  1,500 samples (15.0%)\n"
     ]
    }
   ],
   "source": [
    "# Split data: 70% train, 15% validation, 15% test\n",
    "X_temp, X_test, y_temp, y_test, sku_temp, sku_test = train_test_split(\n",
    "    X, y, sku_ids, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val, sku_train, sku_val = train_test_split(\n",
    "    X_temp, y_temp, sku_temp, test_size=0.176, random_state=42  # 0.176 * 0.85 ≈ 0.15\n",
    ")\n",
    "\n",
    "print(f\"Data split:\")\n",
    "print(f\"  Train: {len(X_train):,} samples ({len(X_train)/n_samples:.1%})\")\n",
    "print(f\"  Val:   {len(X_val):,} samples ({len(X_val)/n_samples:.1%})\")\n",
    "print(f\"  Test:  {len(X_test):,} samples ({len(X_test)/n_samples:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017793c",
   "metadata": {},
   "source": [
    "## 4. Model 1: Intermittent Demand (Two-Stage Prediction)\n",
    "\n",
    "This mode is ideal for sparse/intermittent demand with high zero rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6eed12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Building DeepSequence PWL Model]\n",
      "  SKUs: 25, Features: 6\n",
      "  Activation: mish\n",
      "  ID embedding: 8D\n",
      "  Component hidden: 32\n",
      "  Trend features: 2 indices\n",
      "  Seasonal features: 2 indices\n",
      "  Holiday feature: None\n",
      "  Regressor features: 2 indices\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"holiday_distance_extract\" (type Lambda).\n\nunsupported operand type(s) for +: 'NoneType' and 'int'\n\nCall arguments received by layer \"holiday_distance_extract\" (type Lambda):\n  • inputs=tf.Tensor(shape=(None, 6), dtype=float32)\n  • mask=None\n  • training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      2\u001b[39m model_intermittent = DeepSequencePWL(\n\u001b[32m      3\u001b[39m     num_skus=n_skus,\n\u001b[32m      4\u001b[39m     n_features=n_features,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     activation=\u001b[33m'\u001b[39m\u001b[33mmish\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Build the model with feature separation (NEW in modular architecture)\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Each component receives only its relevant features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m main_model, trend_model, seasonal_model, holiday_model, regressor_model = \u001b[43mmodel_intermittent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrend_feature_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Time-based features for trend\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseasonal_feature_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Cyclic features for seasonality\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mholiday_feature_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# No holiday feature in this dataset\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregressor_feature_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# External variables\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Intermittent model built with feature separation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Total parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_model.count_params()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-mritunjay.kmr1@gmail.com/My Drive/jubilant/examples/../src/deepsequence_pwl/model.py:392\u001b[39m, in \u001b[36mDeepSequencePWL.build_model\u001b[39m\u001b[34m(self, trend_feature_indices, seasonal_feature_indices, holiday_feature_index, regressor_feature_indices)\u001b[39m\n\u001b[32m    380\u001b[39m seasonal_forecast = Dense(\n\u001b[32m    381\u001b[39m     \u001b[32m1\u001b[39m, \n\u001b[32m    382\u001b[39m     activation=\u001b[33m'\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    383\u001b[39m     use_bias=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    384\u001b[39m     name=\u001b[33m'\u001b[39m\u001b[33mseasonal_forecast\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    385\u001b[39m )(seasonal_out)\n\u001b[32m    387\u001b[39m \u001b[38;5;66;03m# ====================================================================\u001b[39;00m\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# HOLIDAY COMPONENT (PWL + Lattice)\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[38;5;66;03m# ====================================================================\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m    391\u001b[39m \u001b[38;5;66;03m# Extract holiday_distance feature\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m holiday_distance_input = \u001b[43mLambda\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholiday_feature_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43mholiday_feature_index\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mholiday_distance_extract\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m    395\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# PWL calibration: adapt range based on data granularity\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data_frequency == \u001b[33m'\u001b[39m\u001b[33mdaily\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-mritunjay.kmr1@gmail.com/My Drive/jubilant/.venv/lib/python3.13/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/GoogleDrive-mritunjay.kmr1@gmail.com/My Drive/jubilant/examples/../src/deepsequence_pwl/model.py:393\u001b[39m, in \u001b[36mDeepSequencePWL.build_model.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    380\u001b[39m seasonal_forecast = Dense(\n\u001b[32m    381\u001b[39m     \u001b[32m1\u001b[39m, \n\u001b[32m    382\u001b[39m     activation=\u001b[33m'\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    383\u001b[39m     use_bias=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    384\u001b[39m     name=\u001b[33m'\u001b[39m\u001b[33mseasonal_forecast\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    385\u001b[39m )(seasonal_out)\n\u001b[32m    387\u001b[39m \u001b[38;5;66;03m# ====================================================================\u001b[39;00m\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# HOLIDAY COMPONENT (PWL + Lattice)\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[38;5;66;03m# ====================================================================\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m    391\u001b[39m \u001b[38;5;66;03m# Extract holiday_distance feature\u001b[39;00m\n\u001b[32m    392\u001b[39m holiday_distance_input = Lambda(\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x[:, holiday_feature_index:\u001b[43mholiday_feature_index\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m],\n\u001b[32m    394\u001b[39m     name=\u001b[33m'\u001b[39m\u001b[33mholiday_distance_extract\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    395\u001b[39m )(main_input)\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# PWL calibration: adapt range based on data granularity\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data_frequency == \u001b[33m'\u001b[39m\u001b[33mdaily\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: Exception encountered when calling layer \"holiday_distance_extract\" (type Lambda).\n\nunsupported operand type(s) for +: 'NoneType' and 'int'\n\nCall arguments received by layer \"holiday_distance_extract\" (type Lambda):\n  • inputs=tf.Tensor(shape=(None, 6), dtype=float32)\n  • mask=None\n  • training=None"
     ]
    }
   ],
   "source": [
    "# Build model with intermittent handling enabled\n",
    "model_intermittent = DeepSequencePWL(\n",
    "    num_skus=n_skus,\n",
    "    n_features=n_features,\n",
    "    enable_intermittent_handling=True,  # Two-stage prediction\n",
    "    id_embedding_dim=8,\n",
    "    component_hidden_units=32,\n",
    "    component_dropout=0.2,\n",
    "    zero_prob_hidden_units=64,\n",
    "    zero_prob_hidden_layers=2,\n",
    "    zero_prob_dropout=0.2,\n",
    "    activation='mish'\n",
    ")\n",
    "\n",
    "# Build the model with feature separation (NEW in modular architecture)\n",
    "# Each component receives only its relevant features\n",
    "main_model, trend_model, seasonal_model, holiday_model, regressor_model = model_intermittent.build_model(\n",
    "    trend_feature_indices=[0, 1],      # Time-based features for trend\n",
    "    seasonal_feature_indices=[2, 3],   # Cyclic features for seasonality\n",
    "    holiday_feature_index=None,        # No holiday feature in this dataset\n",
    "    regressor_feature_indices=[4, 5]   # External variables\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Intermittent model built with feature separation\")\n",
    "print(f\"  Total parameters: {main_model.count_params():,}\")\n",
    "print(f\"  Outputs: {list(main_model.output.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train\n",
    "main_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'final_forecast': 'mae'},\n",
    "    metrics={'final_forecast': ['mae']}\n",
    ")\n",
    "\n",
    "print(\"Training intermittent model...\\n\")\n",
    "history = main_model.fit(\n",
    "    [X_train, sku_train],\n",
    "    {'final_forecast': y_train},\n",
    "    validation_data=([X_val, sku_val], {'final_forecast': y_val}),\n",
    "    epochs=3,\n",
    "    batch_size=256,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "predictions_intermittent = main_model.predict([X_test, sku_test], verbose=0)\n",
    "\n",
    "y_pred = predictions_intermittent['final_forecast'].flatten()\n",
    "zero_prob = predictions_intermittent['zero_probability'].flatten()\n",
    "\n",
    "test_mae = np.abs(y_test - y_pred).mean()\n",
    "\n",
    "print(f\"\\nIntermittent Model Performance:\")\n",
    "print(f\"  Test MAE: {test_mae:.4f}\")\n",
    "print(f\"  Zero probability range: [{zero_prob.min():.3f}, {zero_prob.max():.3f}]\")\n",
    "print(f\"  Mean zero probability: {zero_prob.mean():.3f}\")\n",
    "print(f\"  Actual zero rate: {(y_test == 0).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb47553",
   "metadata": {},
   "source": [
    "## 5. Model 2: Continuous Demand (Direct Forecast)\n",
    "\n",
    "This mode is ideal for regular continuous demand forecasting with fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfbabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with intermittent handling disabled\n",
    "model_continuous = DeepSequencePWL(\n",
    "    num_skus=n_skus,\n",
    "    n_features=n_features,\n",
    "    enable_intermittent_handling=False,  # Direct forecast only\n",
    "    id_embedding_dim=8,\n",
    "    component_hidden_units=32,\n",
    "    component_dropout=0.2,\n",
    "    activation='mish'\n",
    ")\n",
    "\n",
    "# Build the model with feature separation\n",
    "main_model2, _, _, _, _ = model_continuous.build_model(\n",
    "    trend_feature_indices=[0, 1],\n",
    "    seasonal_feature_indices=[2, 3],\n",
    "    holiday_feature_index=None,\n",
    "    regressor_feature_indices=[4, 5]\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Continuous model built with feature separation\")\n",
    "print(f\"  Total parameters: {main_model2.count_params():,}\")\n",
    "print(f\"  Parameter savings: {(1 - main_model2.count_params()/main_model.count_params())*100:.1f}%\")\n",
    "print(f\"  Outputs: {list(main_model2.output.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75955b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train\n",
    "main_model2.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'final_forecast': 'mae'},\n",
    "    metrics={'final_forecast': ['mae']}\n",
    ")\n",
    "\n",
    "print(\"Training continuous model...\\n\")\n",
    "history2 = main_model2.fit(\n",
    "    [X_train, sku_train],\n",
    "    {'final_forecast': y_train},\n",
    "    validation_data=([X_val, sku_val], {'final_forecast': y_val}),\n",
    "    epochs=3,\n",
    "    batch_size=256,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "predictions_continuous = main_model2.predict([X_test, sku_test], verbose=0)\n",
    "\n",
    "y_pred2 = predictions_continuous['final_forecast'].flatten()\n",
    "\n",
    "test_mae2 = np.abs(y_test - y_pred2).mean()\n",
    "\n",
    "print(f\"\\nContinuous Model Performance:\")\n",
    "print(f\"  Test MAE: {test_mae2:.4f}\")\n",
    "print(f\"  Note: Higher MAE expected for sparse data without intermittent handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111babf",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783220db",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Intermittent (Two-Stage)', 'Continuous (Direct)'],\n",
    "    'Parameters': [main_model.count_params(), main_model2.count_params()],\n",
    "    'Test MAE': [test_mae, test_mae2],\n",
    "    'Best For': ['Sparse/intermittent demand (high zero rate)', 'Regular continuous demand']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n✓ For this dataset (zero rate: {(y_test == 0).mean():.1%}):\")\n",
    "print(f\"  Intermittent model is {test_mae2/test_mae:.1f}x better\")\n",
    "print(f\"  Using {main_model.count_params() - main_model2.count_params():,} more parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b161d47",
   "metadata": {},
   "source": [
    "## 7. Component Analysis (Intermittent Model)\n",
    "\n",
    "Let's examine the individual component contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2023780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample for analysis\n",
    "sample_idx = 0\n",
    "sample_X = X_test[sample_idx:sample_idx+1]\n",
    "sample_sku = sku_test[sample_idx:sample_idx+1]\n",
    "sample_y = y_test[sample_idx]\n",
    "\n",
    "# Get predictions from individual components\n",
    "trend_pred = trend_model.predict([sample_X, sample_sku], verbose=0)[0, 0]\n",
    "seasonal_pred = seasonal_model.predict([sample_X, sample_sku], verbose=0)[0, 0]\n",
    "holiday_pred = holiday_model.predict([sample_X, sample_sku], verbose=0)[0, 0]\n",
    "regressor_pred = regressor_model.predict([sample_X, sample_sku], verbose=0)[0, 0]\n",
    "\n",
    "# Get main model predictions\n",
    "main_pred = main_model.predict([sample_X, sample_sku], verbose=0)\n",
    "base_forecast = main_pred['base_forecast'][0, 0]\n",
    "final_forecast = main_pred['final_forecast'][0, 0]\n",
    "zero_prob = main_pred['zero_probability'][0, 0]\n",
    "\n",
    "print(f\"\\nComponent Analysis for Sample {sample_idx}:\")\n",
    "print(f\"  Trend:       {trend_pred:8.4f}\")\n",
    "print(f\"  Seasonal:    {seasonal_pred:8.4f}\")\n",
    "print(f\"  Holiday:     {holiday_pred:8.4f}\")\n",
    "print(f\"  Regressor:   {regressor_pred:8.4f}\")\n",
    "print(f\"  \" + \"-\" * 30)\n",
    "print(f\"  Base forecast:  {base_forecast:8.4f}\")\n",
    "print(f\"  Zero prob:      {zero_prob:8.4f}\")\n",
    "print(f\"  Final forecast: {final_forecast:8.4f}\")\n",
    "print(f\"  Actual value:   {sample_y:8.4f}\")\n",
    "print(f\"  Error:          {abs(final_forecast - sample_y):8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b959eb",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Intermittent Mode** (`enable_intermittent_handling=True`):\n",
    "   - Includes zero probability network\n",
    "   - Best for sparse/intermittent demand (high zero rate)\n",
    "   - More parameters but better accuracy for sparse data\n",
    "\n",
    "2. **Continuous Mode** (`enable_intermittent_handling=False`):\n",
    "   - Direct forecast without zero probability overhead\n",
    "   - Best for regular continuous demand\n",
    "   - 86% fewer parameters, more efficient\n",
    "\n",
    "3. **Components**:\n",
    "   - Trend: Captures long-term patterns\n",
    "   - Seasonal: Captures periodic patterns\n",
    "   - Holiday: Captures special events (PWL + Lattice)\n",
    "   - Regressor: Captures feature relationships\n",
    "   - All combined additively for interpretability\n",
    "\n",
    "### When to Use:\n",
    "\n",
    "- **High zero rate (>70%)**: Use intermittent mode\n",
    "- **Low zero rate (<30%)**: Use continuous mode\n",
    "- **Mixed scenarios**: Test both and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d3c3b",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Try with your own data\n",
    "2. Tune hyperparameters (hidden units, dropout, layers)\n",
    "3. Experiment with different activations ('mish', 'relu', 'elu')\n",
    "4. Train for more epochs with early stopping\n",
    "5. Analyze component contributions for insights\n",
    "\n",
    "See `deepsequence/deepsequence_pwl/README.md` for detailed documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
